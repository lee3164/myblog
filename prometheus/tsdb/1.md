# tsdb的写入
##  几个基本概念
- sample timestamp, value结构，代表的是二维平面的一个个点，横轴是时间，纵轴是value
- series 代表的是prometheus的一条数据中的labels，可以理解为samples组成的曲线的名称，和samples的关系是1:M\
- ref series的id
- chunk 存储sample的数据块，sample是按照时间顺序存储，每个chunk都会存储某个时间段内的数据
- posting labels中的符号和ref所构成的倒排索引，用于根据符号快速得到series
- wal write ahead log, 也就是redo log

```golang
type DB struct {
	dir   string
	lockf fileutil.Releaser

	logger    log.Logger
	metrics   *dbMetrics
	opts      *Options
	chunkPool chunkenc.Pool
	compactor Compactor

	// Mutex for that must be held when modifying the general block layout.
	mtx    sync.RWMutex
	blocks []*Block

	head *Head

	compactc chan struct{}
	donec    chan struct{}
	stopc    chan struct{}

	// cmtx ensures that compactions and deletions don't run simultaneously.
	cmtx sync.Mutex

	// autoCompactMtx ensures that no compaction gets triggered while
	// changing the autoCompact var.
	autoCompactMtx sync.Mutex
	autoCompact    bool

	// Cancel a running compaction when a shutdown is initiated.
	compactCancel context.CancelFunc
}

type Head struct {
	// Keep all 64bit atomically accessed variables at the top of this struct.
	// See https://golang.org/pkg/sync/atomic/#pkg-note-BUG for more info.
	chunkRange       int64  // 每个chunk能够存储的数据时间范围
	numSeries        uint64 
	minTime, maxTime int64 // Current min and max of the samples included in the head.
	minValidTime     int64 // Mint allowed to be added to the head. It shouldn't be lower than the maxt of the last persisted block.
	lastSeriesID     uint64

	metrics    *headMetrics
	wal        *wal.WAL // wal日志
	logger     log.Logger
	appendPool sync.Pool
	seriesPool sync.Pool
	bytesPool  sync.Pool

	// All series addressable by their ID or hash.
	series *stripeSeries

	symMtx  sync.RWMutex
	symbols map[string]struct{}
	values  map[string]stringset // Label names to possible values.

	deletedMtx sync.Mutex
	deleted    map[uint64]int // Deleted series, and what WAL segment they must be kept until.

	postings *index.MemPostings // Postings lists for terms. 符号表倒排索引

	tombstones *tombstones.MemTombstones

	iso *isolation

	cardinalityMutex      sync.Mutex
	cardinalityCache      *index.PostingsStats // Posting stats cache which will expire after 30sec.
	lastPostingsStatsCall time.Duration        // Last posting stats call (PostingsCardinalityStats()) time for caching.
}

type Appender interface {
	Add(l labels.Labels, t int64, v float64) (uint64, error)
	AddFast(ref uint64, t int64, v float64) error
	Commit() error
	Rollback() error
}

```
db没有直接写入的接口，而是提供Appender，供批量写入。每条数据都是由timestamp, value和labels组成。
其中labels是一个kv列表，同一个labels在timestamp维度的value变化构成了时间序列的图，tsdb称之为
series。一个series主要组成为labels和t,v构成的列表。如下：

```golang
type memSeries struct {
	sync.Mutex

	ref          uint64 // 每个labels对应一个id，用于快速查找
	lset         labels.Labels 
	chunks       []*memChunk  //t,v写入文件，构成一个个chunk
	headChunk    *memChunk
	chunkRange   int64 
	firstChunkID int

	nextAt        int64 // Timestamp at which to cut the next chunk.
	sampleBuf     [4]sample
	pendingCommit bool // Whether there are samples waiting to be committed to this series.

	app chunkenc.Appender // Current appender for the chunk.

	txs *txRing
}

type seriesHashmap map[uint64][]*memSeries

type stripeSeries struct {
	size   int
	series []map[uint64]*memSeries
	hashes []seriesHashmap
	locks  []stripeLock
}
```
后者是series的一个分片hash，分片hash的好处在于减小锁冲突，将所带来的的代价降到最低，其中series存的是series的id到series的关系，分片规则用的
是series的id取模。seriesHashmap存的是hash到series的映射。因为hash可能会有冲突，所以用的是一个list，用于hash冲突时的拉链存储，分片规则用的
是hash值取模。

```golang
type MemPostings struct {
	mtx     sync.RWMutex
	m       map[string]map[string][]uint64
	ordered bool
}
```
上面的是结构是倒排索引，用于快速搜索labels对应的seriesId，m中的两个key分别对应label的key和value，int64代表series的id。

## 写入流程
先看看写入的接口定义，如下：

```
type Appender interface {
	Add(l labels.Labels, t int64, v float64) (uint64, error)
	AddFast(ref uint64, t int64, v float64) error
	Commit() error
	Rollback() error
}
```
接口设计就是为了批量写，可以一次性写入多条数据，然后commit，出现问题则rollback。

### Add和AddFast
主要区别在于前者是传入的labels，后者是ref。在确定了ref的情况下用AddFast可以少一些校验的步骤。在Add中主要用于检测labels是否已经创建，没有创建
则需要生成ref，并插入到正排和倒排中，然后返回ref。然后AddFast则将sample，也就是t，v存放到内存中，等待下一步操作。

### Commit
commit 用于提交之前的批量数据。下面是简化的流程：

```
func (a *headAppender) Commit() error {
    // 1. 写入 wal日志，用于出现问题时回放数据
	if err := a.log(); err != nil {
		//nolint: errcheck
		a.Rollback() // Most likely the same error will happen again.
		return errors.Wrap(err, "write to WAL")
	}

	defer a.head.metrics.activeAppenders.Dec()
	defer a.head.putAppendBuffer(a.samples)
	defer a.head.putSeriesBuffer(a.sampleSeries)
	defer a.head.iso.closeAppend(a.appendID)

	total := len(a.samples)
	var series *memSeries
    // 2. 将samples写入chunk中
	for i, s := range a.samples {
		series = a.sampleSeries[i]
		series.Lock()
		ok, chunkCreated := series.append(s.T, s.V, a.appendID)
		series.cleanupAppendIDsBelow(a.cleanupAppendIDsBelow)
		series.pendingCommit = false
		series.Unlock()

		if !ok {
			total--
		}
		if chunkCreated {
			a.head.metrics.chunks.Inc()
			a.head.metrics.chunksCreated.Inc()
		}
	}

	a.head.metrics.samplesAppended.Add(float64(total))
	a.head.updateMinMaxTime(a.mint, a.maxt)

	return nil

```


